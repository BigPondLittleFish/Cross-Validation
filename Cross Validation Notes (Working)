# Cross Validation - a method of estimating the expected prediction error. #
############################################################################

# ---------------------------------------------------------------
# When you implement your model, what will the error rate be? 
# Cross validation allows us to compare different machine learning methods to get a sense
# of how well they work in practice.

# There are three important notes on cross validation... 

# 1. Cross validation will give you an approximation of your error rate. 

# 2.  Helps you select the best fit of your model.

# 3. Helps you ensure that your model is not over-fit.
# ---------------------------------------------------------------

# THERE ARE DIFFERENT TYPES OF CROSS VALIDATION

# Popular Methods:
# ------------------------
# - Hold out method
# - K-fold 
# - Leave one out 
# - Bootstrap methods
# - Logit 
# - SVM
# ------------------------

# Hold out Method 
#################

# - One of the most used/popular method. 
# - Split your data into two subsample, a training sample and a test (hold out) sample.
# - 50-70% of your data in training with 50-30% in test (An industry standard, but not hard and fast rule)
# - Root Mean Squared Error (RMSE), check them for both your test and training samples, if they are very similar,
#   then the model is probably not over fit. If they are very different, you need to go back to rebuild your
#   model and check if your sample is representative and so on.
# - Disadvantage is that it is prone to SAMPLE BIAS, becasue you do not decide which obs are in test and training 
#   samples, it is done through random sampling. 
# - This random sampling is not very representative necessarily, becasue if we change the splitting pattern, 
#   we will probably get a different result. Low chance, but still possible. 


# K-fold 
########

# - Split the sample into K equal size sub samples 
# - k = N/k, wher eeach sub sample has the same number of observations 
# - If you had 5 sub samples, you use one subsample as validation (test) 
#   set and the other 4 as training sets 
# - Use RMSE again to see if there is consistency across 
# - Must do this so that every sub sample has an opportunity to hold as the test subsample.
#   Everytime you get a new RMSE rate. 
# - With K-validation, you will get k-errors
# - Normally, k = 5:10, most Common practice is to use 10-fold cross validations 
# - ADVANTAGE: it does not matter how the data is divided. This problem occurs in the Hold out method. The
#   selection bias will not be present with K-fold. This is the case because... The validation sub sample is 
#   changing, where training and vlaidation sub sets are cycled through and will all be apart of both groups 
#   at one point or another 
# - A larger K will lead to less bias toward overestimating the true expected error (as the number of training folds
#   will be close to the total number of observations in the dataset), but will lead to a higher 
#   variance and runnning time (see "Leave one out").
# - A higher K gives more samples to estimate a more accurate confidence interval on your estimate. 
# - K= 2 may be very inaccurate, but K = 100 may take forever to run and bring a higher variance with it. So K = 10 
#   often plays a "happy medium" where you can increase your accuracy, hopefully without increasing your 
#   variance by a dramatic amount.


# Leave one out 
###############

# K = N, we take out one observation and test it against the others, get the error, 
# then change the observation to another single observation, test your model on that single observations.
# you will have N erorrs for N observations. Summation of all single errors/N 
# - Problem is that this can take a lot of time, ex: 1000 observations -> 1000 iterations 
# - Many computers and systems can effectively handle this. 


# Bootstrap Methods 
###################

# - Randomly draw datasets from the training sample, data points can be redrawn 
# - Each sample is the same size as the training sample
# - Refit the model with boostrap samples
# - Examine the model 
# - Training sample -> select data points with replacement -> N number of bootstrap sample and check for 
#   consistentcy 


# Support Vector Machines
#########################

# - SVM is a binary classification method for Y/N, T/F
#   Classifcaiton can be multi-demnsional.
#   Sometimes only T/F, Y/N, or may have multiple Easy/Med/Hard
#   Classification takes old data trains, and uses new data to predict. 
# - Ex: Stock price prediction ( Will a stock rise or fall?)
# - Ex: The color of the car may not matter, but the length of the car might. 
# - Support vector machine is a type of classficiation algortimh which classifies data based on its features. 
# - Will classify any new element into one of two classes, then when we give it an unknown/new data, 
#   the algorithm should/will classify it into one of the two.
# - When we plot the data, we can see a clear seperation between the two groups. 
#   Although this is not always the case. 
# - Before applying math, we notice a new data point enters into our database, 
#   and we dont know its class. We know it will be one of the two... which one? 
#   What do we do to find out?
#   We need to draw a boundary to classify the new data point!
# - We find the line of best seperation ( the line that best seperate the two groups)
#   Potential lines can place the new data into either category,
#   depending on where the line(s) seperate, so how do we pick?
# - The two "support vectors" (a.k.a the data points) that are closest together and 
#   maximize the distance between the two groups support the algorithm, and that is why 
#   They are "support vectors." 

# - The best line ("boundary") will always have the greatest margin, or rather, distance 
#   between the support vectors ("the boundry of greatest margin")
#   The line that has the largest margin between the two support vectors 
#   (Again, they are technically data points, but we call support vectors because
#   of the dimension we are  working in).
# - The Hyperplane = The boundry of greatest margin, called hyperplane again,
#   becasue of the dimension level we are in is where lines are actually planes. 
# - "D+" is the shortest distnace to the closest positivie point. 
# - "D-" is the shortest distnace to the closest negative point.
# - Distance between them = Distance margin, and we always want to maximize that 
#   If we dont maximize it - > misclassification.
# - If there are two classes (this or that) then it is 2 Dimensional and 
#   we would execute what is called a linear SVM.
# - Remember SVM is a binary classifier, 3 or more classes --> Try random forests instead, 
# - Applications include: face detection, text catergorization, image classification, bioinformatic
# - SVM Kernal:
#   What if no clear linear seperation between two classes? 
#   (Then it is split by a plane)
#   If not linearly seperable, because we cant seperate into two classes 
#   with a normal linear type of line
#   We apply a function, called a kernal function to transform into 
#   a higher dimensional space (e.g. from a 2 dimensional to a 3 dimensional space)
#   If we let R be the number of dimensions of the data, the kernel 
#   converts a given R^2 dimension to R^3 dimension 
#   Once you have transformed you can seperate the data 
#   using our plane (becasue 3D, not a line like in 2D).

# - "3+D" -> seperation occurs as a hyperplane (more complex).




# -----------------------------------------------------------------------------------------------------------
# EXAMPLE/HYPOTHETICAL/LOGICAL STEPS:

# 1. Estimate the parameters for the machine learning methods, "train the algorithm."
#
# 2. Evaluate how well the machine learning methods work. In other words, we need to find out 
#    if the "curve" or whatever method used will do a good job categorizing new data added
#    (think in terms of new patients and predicting heart problems). In machine learning 
#    lingo, evaluating a method is referred to as "testing the algorithm."
# 
# We need the data to train the machine learning methods and test those machine learning methods.
#
# A terrible apporach would be to use all of the data to estimate the parameters (train the algorithm)
# because there would be no data left to test it. 
#
# Ex: WE could use 75% of the data for training and 25% for testing... From there, we could then 
# compare how well each one categorized the test data. 
# 
# But how do we know that using the first 75% of the data for training and the last 25% for testing is the best way 
# to divide up the data? For example, 
# What if we used the first or second 25% of the data for testing and the last 75% for training, would that be better? 
# We dont need to worry about which block to choose for testing, because cross validation will use them all,
# one at a time, and allow for a summary of the results at the end. 
#
# K FOLD CROSS VALIDATION (FOUR FOLD):
# Ex: cross validation would start by using the first 3 blocks (Or quarters, where there would be 4 blocks total) 
# to train and the last block (the last 25% ) to test.
# Then it would keep track of how well the test preformed.
# Then it would use a different/the next quarter to test, and the other three quarters for training. 
# We would do this until every block of data had the chance to be used for testing and can even see 
# what method has the best classification so we can select the best one.
# -----------------------------------------------------------------------------------------------------------


# Tuning Parameters:
# If we wanted to use a method that involved a "tuning parameter" (a parameter that isnt estimated,
# but just sort of guessed) we could use 10-fold cross validation to find the best value for that 
# tuning parameter.
